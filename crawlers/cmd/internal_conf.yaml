applicationName: crawlers

http:
  address: 0.0.0.0
  port: 8080


redis:
  address: localhost:6379
  password: db_pwd
  defaultDb: 0
  poolSize: 10
  poolTimeout: 30 #seconds
  readTimeout: 30 #seconds
  writeTimeout: 30 #seconds
  autoCreateConsumerGroups: true

mongodb:
  uri: "mongodb://db_user:db_pwd@127.0.0.1:27017/crawlers?retryWrites=true&w=majority&authSource=admin&maxPoolSize=10"
  database: crawler
#  database: crawlers

taskPool:
  capacity: 10000

logConfig:
  enabled: true
  logLevel: INFO
  logPath: ./ # 日志存放路径：${logPath}/${fileName}
  outputToConsole: true  # 是否同时将日志打印到控制台
  fileName: crawlers.log
  maxSizeInMB: 5   # 日志文件的体积
  maxAgeInDays: 5   # 最多保留天数
  maxBackups: 2    # 允许存在几个日志备份文件
  compress: true    # 是否压缩保存历史文件


webSites:
  - name: onej #最高支持catalogPage， catalog为人员名称
    regexSettings:
      #拼装每一页url
      parsePageRegex: page=([^\&]+)
      pagePrefix: "page="
    mongoCollections:
      novel: novel
      catalogPage: catalogPage
    attributes:
      directory: /home/cloud/Desktop/onej/
      consumers: 3

  - name: nsf
    regexSettings:
      #拼装每一页url
      parsePageRegex: ([^/]+)\.html
      pagePrefix: ""
      pageSuffix: ".html"
    crawlerSettings:
      catalog:
        skipIfPresent: false
        skipSaveIfPresent: true
      catalogPage:
        skipIfPresent: false
        skipSaveIfPresent: true
      novel:
        skipIfPresent: false
        skipSaveIfPresent: true
      chapter:
        enabled: true
        skipIfPresent: true
        skipSaveIfPresent: true
